# TODO: Pattern Dispatch Enhancements and Instrumentation

This document tracks tasks for improving the current pattern-based dispatch system.

## Table of Contents
1. [Documentation Updates](#documentation-updates)
2. [Timing Instrumentation](#timing-instrumentation)
3. [Pattern System Enhancements](#pattern-system-enhancements)
4. [Cleanup Tasks](#cleanup-tasks)

---

## Documentation Updates

### ‚úÖ Completed
- [x] Created REDUNDANT.md with deprecated features
- [x] Created REFERENCE_NEW.md with current system documentation
- [x] Updated README.md to reflect pattern-based system
- [x] Updated MULTI_WRITER.md usage examples

### üîÑ In Progress
- [ ] Replace old REFERENCE.md with REFERENCE_NEW.md
- [ ] Archive or update THREAD_ORGANIZATION.md (describes old hard-coded system)
- [ ] Update TODO.md to reflect current codebase status (this file)

### üìã Pending
- [ ] Create PATTERN_GUIDE.md with:
  - Pattern design best practices
  - Common pattern templates
  - Debugging tips for pattern validation errors
- [ ] Create EXAMPLES.md showcasing:
  - Basic release-acquire test
  - Isolated acquire per CTA
  - Cross-scope visibility tests
  - Multi-writer concurrent patterns
- [ ] Add inline code documentation:
  - Doxygen-style comments for all consumer functions
  - Document dispatch flow in detail
  - Add examples to pattern_config.hpp

---

## Timing Instrumentation

### Current Status
The framework has consumer functions but **no timing instrumentation** to measure propagation delays.

### Goals
1. Measure per-thread read/write latencies
2. Track flag propagation times
3. Identify which flag triggered first in multi-flag readers
4. Output parseable timing data for analysis

### Implementation Plan

#### Phase 1: Timing Infrastructure

**Data Structures:**
```c
// Per-thread GPU timing data
typedef struct gpu_timing_data {
    clock_t start_time;
    clock_t end_time;
    clock_t flag_trigger_time;
    uint32_t thread_id;
    uint32_t consumer_type;  // writer, reader_acq, reader_rlx
    uint32_t flag_type;      // thread, block, device, system
    char padding[PAGE_SIZE - sizeof(clock_t)*3 - sizeof(uint32_t)*3];
} gpu_timing_data;

// Per-thread CPU timing data
typedef struct cpu_timing_data {
    uint64_t start_ns;
    uint64_t end_ns;
    uint64_t flag_trigger_ns;
    uint32_t thread_id;
    uint32_t consumer_type;
    uint32_t flag_type;
    char padding[PAGE_SIZE - sizeof(uint64_t)*3 - sizeof(uint32_t)*3];
} cpu_timing_data;
```

**Allocation:**
- Allocate timing arrays in main()
- Pass timing pointers to dispatch functions
- Copy GPU timing data to host after synchronization

#### Phase 2: Instrument Consumer Functions

**GPU Readers:**
```cuda
template <typename B, typename W, typename R>
__device__ void gpu_buffer_reader_acquire(
    B *buffer, bufferElement_na *results,
    R *r_signal, W *w_signal, flag_s *fallback_signal,
    gpu_timing_data *timing  // ADD THIS PARAMETER
) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    timing[tid].start_time = clock64();
    timing[tid].thread_id = tid;
    
    // ... existing code ...
    
    // Time flag wait
    clock_t flag_start = clock64();
    while(w_signal->flag.load(cuda::memory_order_acquire) == 0 && ...) {}
    timing[tid].flag_trigger_time = clock64();
    
    // ... read buffer ...
    
    timing[tid].end_time = clock64();
}
```

**GPU Writers:**
```cuda
template <typename B, typename W, typename R>
__device__ void gpu_buffer_writer_release(
    B *buffer, R *r_signal, W *w_signal, flag_s *fallback_signal,
    gpu_timing_data *timing  // ADD THIS
) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    timing[tid].start_time = clock64();
    
    // ... existing write code ...
    
    w_signal->flag.store(1, cuda::memory_order_release);
    timing[tid].flag_trigger_time = clock64();
    
    // ... sleep ...
    
    timing[tid].end_time = clock64();
}
```

**CPU Functions:** Use `std::chrono::high_resolution_clock::now()`

#### Phase 3: Output and Analysis

**Output Format (CSV):**
```csv
device,block,thread,role,ordering,scope,watch_flag,start_ns,flag_ns,end_ns,result
gpu,0,0,writer,release,device,,1234567890,1234567900,1234568000,512
gpu,0,1,reader,acquire,device,device,1234567890,1234567905,1234568010,512
gpu,0,2,reader,relaxed,device,device,1234567890,1234567920,1234568025,512
```

**Analysis Tools:**
- Add to `scripts/analyze_timing.py`
- Calculate per-thread latencies
- Compare acquire vs. relaxed propagation times
- Visualize flag propagation across thread hierarchy

#### Phase 4: Regression Testing

- Add timing baseline tests
- Detect performance regressions
- Compare across memory allocators
- Track timing across different GPU architectures

---

## Pattern System Enhancements

### Configuration Features

#### ‚úÖ Implemented
- [x] YAML-based pattern configuration
- [x] Per-thread role assignment
- [x] Multi-writer mode with validation
- [x] Block and thread range syntax
- [x] Pattern validation on load

#### üìã Planned

**1. Pattern Composition**
```yaml
# Import and extend existing patterns
patterns:
  - name: "extended_test"
    base: "isolated_acquire_per_cta"
    
    overrides:
      gpu:
        blocks:
          block_3:
            threads_0_31: {role: reader, ordering: acquire, watch_flag: system}
```

**2. Variable Substitution**
```yaml
# Define reusable values
variables:
  main_scope: device
  reader_ordering: acquire

patterns:
  - name: "parameterized_test"
    gpu:
      blocks:
        block_0:
          thread_0: {role: writer, scope: ${main_scope}, ordering: release}
          threads_1_31: {role: reader, ordering: ${reader_ordering}, watch_flag: ${main_scope}}
```

**3. Pattern Validation Levels**
```yaml
patterns:
  - name: "experimental_pattern"
    validation_level: permissive  # strict, permissive, none
    
    # Allow experimental configurations
```

**4. Dynamic Thread Counts**
```yaml
patterns:
  - name: "scalable_test"
    
    gpu:
      num_blocks: ${GPU_NUM_BLOCKS}  # From environment or command-line
      threads_per_block: ${GPU_THREADS_PER_BLOCK}
```

**5. Conditional Configuration**
```yaml
patterns:
  - name: "conditional_test"
    
    gpu:
      blocks:
        block_0:
          if: ${ENABLE_WRITER}
            thread_0: {role: writer, scope: device, ordering: release}
          else:
            thread_0: {role: dummy_writer}
```

### Runtime Features

**1. Pattern Profiling**
- Automatic performance profiling per pattern
- Track execution time, synchronization delays
- Output: `pattern_name_profile.json`

**2. Pattern Debugging**
- Verbose mode: `-v` flag
- Print thread dispatch decisions
- Track flag state changes
- Output synchronization timeline

**3. Pattern Verification**
- Dry-run mode: `--dry-run`
- Validate pattern without execution
- Print thread assignment table
- Check for potential deadlocks

**4. Interactive Pattern Testing**
```bash
# Interactive REPL for pattern development
./cache_invalidation_testing --interactive -F configs/test.yaml
> load pattern isolated_acquire
> show blocks
> modify block_1 thread_0 ordering=relaxed
> run
> show results
```

---

## Cleanup Tasks

### Code Organization

#### ‚úÖ Completed
- [x] Moved legacy code to stale_code.cuh
- [x] Separated pattern dispatch into dedicated headers
- [x] Created modular pattern configuration system

#### üìã Pending

**1. File Restructuring**
```
Current:
  include/pattern_dispatch.cuh        (GPU functions + dispatch)
  include/pattern_dispatch_cpu.hpp    (CPU functions)

Proposed:
  include/
    dispatch/
      gpu_dispatch.cuh          # GPU dispatch logic
      cpu_dispatch.hpp          # CPU dispatch logic
    consumers/
      gpu_consumers.cuh         # GPU reader/writer functions
      cpu_consumers.hpp         # CPU reader/writer functions
    config/
      pattern_config.hpp        # Configuration structures
      pattern_registry.hpp      # Registry and YAML parsing
```

**2. Remove Unused Defines**
- Remove commented CUDA_THREAD_SCOPE defines from types.hpp
- Clean up old P_H_FLAG_STORE_ORDER / C_H_FLAG_LOAD_ORDER (now in YAML)

**3. Standardize Naming**
- Consistent naming: `gpu_*` for device functions, `cpu_*` for host
- Remove misleading names (e.g., `*_diverge` functions that don't test divergence)

**4. Error Handling**
- Add CUDA error checking after all device operations
- Better error messages for pattern validation failures
- Graceful handling of invalid YAML syntax

### Testing Infrastructure

**1. Unit Tests**
- Test pattern parsing with various YAML formats
- Test pattern validation logic
- Test thread assignment calculation

**2. Integration Tests**
- Automated pattern test suite
- Run all configs/*.yaml patterns
- Verify expected result values

**3. CI/CD**
- GitHub Actions workflow
- Build all data size variants
- Run basic pattern tests
- Check documentation consistency

### Performance Optimization

**1. Constant Memory Usage**
- Currently: Full pattern copied to device constant memory
- Optimize: Only copy active block configurations

**2. Launch Configuration**
- Experiment with different block/thread counts
- Benchmark optimal configuration per pattern type

**3. Synchronization Efficiency**
- Evaluate spin-wait vs. sleep in flag polling
- Optimize fallback timeout values

---

## Long-Term Goals

### Multi-Device Support
- Extend patterns to multiple GPUs
- Test cross-device cache coherence
- Support heterogeneous GPU systems (e.g., H100 + A100)

### Advanced Patterns
- Producer-consumer queues
- Read-modify-write patterns
- Multi-stage pipeline patterns
- Hierarchical synchronization (block‚Üídevice‚Üísystem)

### Analysis Framework
- Statistical analysis of timing data
- Automated bottleneck detection
- Visualization dashboards
- Export to performance analysis tools (NVIDIA Nsight, VTune)

### Documentation
- Video tutorials for pattern creation
- Interactive web-based pattern builder
- Gallery of validated patterns with use cases
- Research paper references and citations

---

## Migration Notes

### Removed from Old TODO.md

The following items from the old TODO.md are **no longer applicable** because the code they referenced has been moved to `stale_code.cuh`:

- ‚ùå Remove simple reader/writer functions
- ‚ùå Remove propagation hierarchy functions  
- ‚ùå Remove CPU reader/writer variants
- ‚ùå Remove commented code blocks
- ‚ùå Fix bufferElement_d scope mismatch (not used in current system)
- ‚ùå Instrument old propagation hierarchy functions

These are documented in [REDUNDANT.md](REDUNDANT.md) for historical reference.

### Still Relevant from Old TODO

The timing implementation plan is still relevant and has been updated above to work with the current pattern dispatch system.

---

## Contributing

When working on TODOs:

1. **Mark Progress:** Update checkboxes as you complete tasks
2. **Document Changes:** Add notes on implementation decisions
3. **Update Tests:** Add tests for new features
4. **Review Docs:** Update relevant documentation files
5. **Clean Code:** Follow existing style and naming conventions

## Priority Legend

- üî¥ **Critical:** Blocks other work or affects correctness
- üü° **High:** Important for usability or performance
- üü¢ **Medium:** Nice to have, improves quality
- üîµ **Low:** Future enhancement, not urgent

Current priorities:
- üî¥ Timing instrumentation (Phase 1-2)
- üî¥ Replace old REFERENCE.md
- üü° Pattern debugging features
- üü° Error handling improvements
- üü¢ Pattern composition
- üîµ Multi-device support
